{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c43bbd-5d2f-45a7-9d6f-a9fc676b1e15",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf3c892-f899-40e5-a3f6-fb55bf37f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import os\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rs = 1234\n",
    "np.random.seed(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96218cd6-3757-4c7b-a5b4-bbfa306e4347",
   "metadata": {},
   "source": [
    "## Loading MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd5ab1a3-eb94-44e4-b941-37a4d0efe89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = MNIST('./', download=True,\n",
    "                 train=True,\n",
    "                 transform=transform)\n",
    "testset = MNIST('./', download=True,\n",
    "                 train=False,\n",
    "                 transform=transform)\n",
    "dataset = ConcatDataset([trainset, testset])\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=256, \n",
    "                                         shuffle=True,\n",
    "                                         num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04b913a-ae78-46c0-95b3-6ed4fc451e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = trainset.data.numpy().reshape(60000, 784)\n",
    "X_test = testset.data.numpy().reshape(10000, 784)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d54bb60-0502-404b-855e-124bb3b115ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "y_test = np.array(testset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e088416a-a7f7-4c8b-abd3-c09138017a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([y_train, y_test])\n",
    "X = np.concatenate([X_train, X_test])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ebe3a-a2e1-4061-83f6-2395d3c0ce25",
   "metadata": {},
   "source": [
    "## Baseline KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8a01566-2f34-4f60-a6d9-d8dd845dd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "# Use the actual number of clusters as parameter\n",
    "n_clusters = len(np.unique(y))\n",
    "\n",
    "# Apply kmeans using sklearn\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=rs)\n",
    "\n",
    "# Get training predictions\n",
    "y_pred_train = kmeans.fit_predict(X_train)\n",
    "\n",
    "# Predictions on unseen test data\n",
    "y_pred_test = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "552227a5-4b31-4556-be4c-443eba2b5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "def clustering_accuracy(y_pred, y_true) -> float:\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    D = D.astype(np.int64)\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    # Confusion matrix.\n",
    "    for i in range(len(y_pred)):\n",
    "        w[int(y_pred[i]), int(y_true[i])] += 1\n",
    "    ind = linear_assignment(-w)\n",
    "    acc = np.sum([w[i, j] for i, j in zip(ind[0], ind[1])]) * 1.0 / y_pred.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483bc17-eab3-43cb-890c-4760b37a5497",
   "metadata": {},
   "source": [
    "### Evaluate KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2494d11c-0d06-4e7a-b4ab-4ef38f7180de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:\n",
      "AMI: 0.4810648182532282\n",
      "ARI: 0.3596336221786749\n",
      "Accuracy: 0.54115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "print(\"Training accuracy:\")\n",
    "print(f\"AMI: {adjusted_mutual_info_score(y_train, y_pred_train)}\")\n",
    "print(f\"ARI: {adjusted_rand_score(y_train, y_pred_train)}\")\n",
    "print(f\"Accuracy: {clustering_accuracy(y_train, y_pred_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "180e4247-8acb-4d31-94b7-c5b266489d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:\n",
      "AMI: 0.4918696527602774\n",
      "ARI: 0.3664993305378044\n",
      "Accuracy: 0.5485\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\")\n",
    "print(f\"AMI: {adjusted_mutual_info_score(y_test, y_pred_test)}\")\n",
    "print(f\"ARI: {adjusted_rand_score(y_test, y_pred_test)}\")\n",
    "print(f\"Accuracy: {clustering_accuracy(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911c685-bff4-48f4-93fa-fa5afa20ce5e",
   "metadata": {},
   "source": [
    "### AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03bc9e67-81b1-42a5-9aa4-181b3eb5e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: int, \n",
    "                hidden_sizes: Tuple[int],\n",
    "                 dropout_rate: float=0.2,\n",
    "                 activation=nn.ReLU()\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.input_layer = torch.nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.n_layers = 0\n",
    "        for i in range(0, len(hidden_sizes) -1):\n",
    "            setattr(self, f\"dense_{i}\", torch.nn.Linear(hidden_sizes[i],\n",
    "                                                        hidden_sizes[i+1])\n",
    "                   )\n",
    "            self.n_layers += 1\n",
    "        self.activation = activation\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout  = nn.Dropout(dropout_rate)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for i in range(0, self.n_layers -1):\n",
    "            x = self.activation(getattr(self, f\"dense_{i}\")(x))\n",
    "            x = self.dropout(x)\n",
    "        # Use layer without activation function\n",
    "        output_layer = getattr(self, f\"dense_{self.n_layers-1}\")\n",
    "        return output_layer(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 activation=nn.ReLU()\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.hidden_sizes = encoder.hidden_sizes\n",
    "        n_layers = encoder.n_layers\n",
    "        self.hidden_sizes = self.hidden_sizes[::-1]\n",
    "        # Reversed order -> dense_0 will be the first to apply here\n",
    "        for i in range(0, n_layers):\n",
    "            setattr(self, f\"dense_{i}\", torch.nn.Linear(self.hidden_sizes[i],\n",
    "                                                        self.hidden_sizes[i+1])\n",
    "                   )\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_sizes[-1],\n",
    "                                                        encoder.input_size)\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.dropout  = nn.Dropout(encoder.dropout_rate)\n",
    "\n",
    "        \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        for i in range(0, self.n_layers):\n",
    "            dense_i = getattr(self, f\"dense_{i}\")\n",
    "            x = dense_i(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size: int, \n",
    "                hidden_sizes: Tuple[int],  dropout_rate: float=0.2,\n",
    "                activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_sizes, dropout_rate)\n",
    "        self.decoder = Decoder(self.encoder)\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor]:\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f527de-bdb3-4524-a66f-3837ee03ed42",
   "metadata": {},
   "source": [
    "### Pre-train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ce9d1e3-ffb1-4cf3-95dc-79a60586a70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch [1 / 300]: 0.33089\n",
      "Loss at epoch [2 / 300]: 0.25614\n",
      "Loss at epoch [3 / 300]: 0.246\n",
      "Loss at epoch [4 / 300]: 0.23487\n",
      "Loss at epoch [5 / 300]: 0.21673\n",
      "Loss at epoch [6 / 300]: 0.20144\n",
      "Loss at epoch [7 / 300]: 0.17995\n",
      "Loss at epoch [8 / 300]: 0.16632\n",
      "Loss at epoch [9 / 300]: 0.15798\n",
      "Loss at epoch [10 / 300]: 0.15158\n",
      "Loss at epoch [11 / 300]: 0.14644\n",
      "Loss at epoch [12 / 300]: 0.14161\n",
      "Loss at epoch [13 / 300]: 0.13742\n",
      "Loss at epoch [14 / 300]: 0.1342\n",
      "Loss at epoch [15 / 300]: 0.13129\n",
      "Loss at epoch [16 / 300]: 0.12832\n",
      "Loss at epoch [17 / 300]: 0.12544\n",
      "Loss at epoch [18 / 300]: 0.12306\n",
      "Loss at epoch [19 / 300]: 0.12088\n",
      "Loss at epoch [20 / 300]: 0.11902\n",
      "Loss at epoch [21 / 300]: 0.11706\n",
      "Loss at epoch [22 / 300]: 0.11526\n",
      "Loss at epoch [23 / 300]: 0.11361\n",
      "Loss at epoch [24 / 300]: 0.11218\n",
      "Loss at epoch [25 / 300]: 0.11086\n",
      "Loss at epoch [26 / 300]: 0.10958\n",
      "Loss at epoch [27 / 300]: 0.10852\n",
      "Loss at epoch [28 / 300]: 0.10749\n",
      "Loss at epoch [29 / 300]: 0.10645\n",
      "Loss at epoch [30 / 300]: 0.10562\n",
      "Loss at epoch [31 / 300]: 0.10477\n",
      "Loss at epoch [32 / 300]: 0.1039\n",
      "Loss at epoch [33 / 300]: 0.10318\n",
      "Loss at epoch [34 / 300]: 0.10233\n",
      "Loss at epoch [35 / 300]: 0.10169\n",
      "Loss at epoch [36 / 300]: 0.10099\n",
      "Loss at epoch [37 / 300]: 0.10042\n",
      "Loss at epoch [38 / 300]: 0.09982\n",
      "Loss at epoch [39 / 300]: 0.09917\n",
      "Loss at epoch [40 / 300]: 0.09864\n",
      "Loss at epoch [41 / 300]: 0.09809\n",
      "Loss at epoch [42 / 300]: 0.09761\n",
      "Loss at epoch [43 / 300]: 0.09718\n",
      "Loss at epoch [44 / 300]: 0.09665\n",
      "Loss at epoch [45 / 300]: 0.09624\n",
      "Loss at epoch [46 / 300]: 0.09575\n",
      "Loss at epoch [47 / 300]: 0.09529\n",
      "Loss at epoch [48 / 300]: 0.09492\n",
      "Loss at epoch [49 / 300]: 0.09452\n",
      "Loss at epoch [50 / 300]: 0.09407\n",
      "Loss at epoch [51 / 300]: 0.0937\n",
      "Loss at epoch [52 / 300]: 0.09341\n",
      "Loss at epoch [53 / 300]: 0.09314\n",
      "Loss at epoch [54 / 300]: 0.09271\n",
      "Loss at epoch [55 / 300]: 0.09239\n",
      "Loss at epoch [56 / 300]: 0.09204\n",
      "Loss at epoch [57 / 300]: 0.09169\n",
      "Loss at epoch [58 / 300]: 0.09146\n",
      "Loss at epoch [59 / 300]: 0.09112\n",
      "Loss at epoch [60 / 300]: 0.09085\n",
      "Loss at epoch [61 / 300]: 0.09056\n",
      "Loss at epoch [62 / 300]: 0.09022\n",
      "Loss at epoch [63 / 300]: 0.09001\n",
      "Loss at epoch [64 / 300]: 0.08975\n",
      "Loss at epoch [65 / 300]: 0.08943\n",
      "Loss at epoch [66 / 300]: 0.08924\n",
      "Loss at epoch [67 / 300]: 0.089\n",
      "Loss at epoch [68 / 300]: 0.08879\n",
      "Loss at epoch [69 / 300]: 0.08844\n",
      "Loss at epoch [70 / 300]: 0.08833\n",
      "Loss at epoch [71 / 300]: 0.08807\n",
      "Loss at epoch [72 / 300]: 0.08785\n",
      "Loss at epoch [73 / 300]: 0.08762\n",
      "Loss at epoch [74 / 300]: 0.08742\n",
      "Loss at epoch [75 / 300]: 0.08716\n",
      "Loss at epoch [76 / 300]: 0.087\n",
      "Loss at epoch [77 / 300]: 0.08673\n",
      "Loss at epoch [78 / 300]: 0.08653\n",
      "Loss at epoch [79 / 300]: 0.08636\n",
      "Loss at epoch [80 / 300]: 0.0862\n",
      "Loss at epoch [81 / 300]: 0.08596\n",
      "Loss at epoch [82 / 300]: 0.08584\n",
      "Loss at epoch [83 / 300]: 0.0857\n",
      "Loss at epoch [84 / 300]: 0.08543\n",
      "Loss at epoch [85 / 300]: 0.08527\n",
      "Loss at epoch [86 / 300]: 0.08515\n",
      "Loss at epoch [87 / 300]: 0.0849\n",
      "Loss at epoch [88 / 300]: 0.0848\n",
      "Loss at epoch [89 / 300]: 0.0846\n",
      "Loss at epoch [90 / 300]: 0.08444\n",
      "Loss at epoch [91 / 300]: 0.08434\n",
      "Loss at epoch [92 / 300]: 0.08421\n",
      "Loss at epoch [93 / 300]: 0.08397\n",
      "Loss at epoch [94 / 300]: 0.08377\n",
      "Loss at epoch [95 / 300]: 0.08366\n",
      "Loss at epoch [96 / 300]: 0.08358\n",
      "Loss at epoch [97 / 300]: 0.08342\n",
      "Loss at epoch [98 / 300]: 0.08328\n",
      "Loss at epoch [99 / 300]: 0.08319\n",
      "Loss at epoch [100 / 300]: 0.08303\n",
      "Loss at epoch [101 / 300]: 0.08262\n",
      "Loss at epoch [102 / 300]: 0.08255\n",
      "Loss at epoch [103 / 300]: 0.08247\n",
      "Loss at epoch [104 / 300]: 0.08249\n",
      "Loss at epoch [105 / 300]: 0.08248\n",
      "Loss at epoch [106 / 300]: 0.08244\n",
      "Loss at epoch [107 / 300]: 0.08238\n",
      "Loss at epoch [108 / 300]: 0.08242\n",
      "Loss at epoch [109 / 300]: 0.08241\n",
      "Loss at epoch [110 / 300]: 0.08237\n",
      "Loss at epoch [111 / 300]: 0.0823\n",
      "Loss at epoch [112 / 300]: 0.0823\n",
      "Loss at epoch [113 / 300]: 0.0823\n",
      "Loss at epoch [114 / 300]: 0.08231\n",
      "Loss at epoch [115 / 300]: 0.08224\n",
      "Loss at epoch [116 / 300]: 0.08222\n",
      "Loss at epoch [117 / 300]: 0.08222\n",
      "Loss at epoch [118 / 300]: 0.08217\n",
      "Loss at epoch [119 / 300]: 0.08223\n",
      "Loss at epoch [120 / 300]: 0.08219\n",
      "Loss at epoch [121 / 300]: 0.08221\n",
      "Loss at epoch [122 / 300]: 0.08214\n",
      "Loss at epoch [123 / 300]: 0.08214\n",
      "Loss at epoch [124 / 300]: 0.08219\n",
      "Loss at epoch [125 / 300]: 0.08218\n",
      "Loss at epoch [126 / 300]: 0.08208\n",
      "Loss at epoch [127 / 300]: 0.08209\n",
      "Loss at epoch [128 / 300]: 0.08204\n",
      "Loss at epoch [129 / 300]: 0.08205\n",
      "Loss at epoch [130 / 300]: 0.0821\n",
      "Loss at epoch [131 / 300]: 0.08198\n",
      "Loss at epoch [132 / 300]: 0.082\n",
      "Loss at epoch [133 / 300]: 0.082\n",
      "Loss at epoch [134 / 300]: 0.08198\n",
      "Loss at epoch [135 / 300]: 0.0819\n",
      "Loss at epoch [136 / 300]: 0.082\n",
      "Loss at epoch [137 / 300]: 0.08188\n",
      "Loss at epoch [138 / 300]: 0.08191\n",
      "Loss at epoch [139 / 300]: 0.08195\n",
      "Loss at epoch [140 / 300]: 0.08189\n",
      "Loss at epoch [141 / 300]: 0.08188\n",
      "Loss at epoch [142 / 300]: 0.08185\n",
      "Loss at epoch [143 / 300]: 0.08188\n",
      "Loss at epoch [144 / 300]: 0.0818\n",
      "Loss at epoch [145 / 300]: 0.08184\n",
      "Loss at epoch [146 / 300]: 0.08175\n",
      "Loss at epoch [147 / 300]: 0.08182\n",
      "Loss at epoch [148 / 300]: 0.08178\n",
      "Loss at epoch [149 / 300]: 0.08177\n",
      "Loss at epoch [150 / 300]: 0.08167\n",
      "Loss at epoch [151 / 300]: 0.08176\n",
      "Loss at epoch [152 / 300]: 0.08173\n",
      "Loss at epoch [153 / 300]: 0.08166\n",
      "Loss at epoch [154 / 300]: 0.08177\n",
      "Loss at epoch [155 / 300]: 0.0816\n",
      "Loss at epoch [156 / 300]: 0.08174\n",
      "Loss at epoch [157 / 300]: 0.08166\n",
      "Loss at epoch [158 / 300]: 0.08165\n",
      "Loss at epoch [159 / 300]: 0.08167\n",
      "Loss at epoch [160 / 300]: 0.0817\n",
      "Loss at epoch [161 / 300]: 0.0816\n",
      "Loss at epoch [162 / 300]: 0.08153\n",
      "Loss at epoch [163 / 300]: 0.08162\n",
      "Loss at epoch [164 / 300]: 0.08161\n",
      "Loss at epoch [165 / 300]: 0.08156\n",
      "Loss at epoch [166 / 300]: 0.08152\n",
      "Loss at epoch [167 / 300]: 0.08152\n",
      "Loss at epoch [168 / 300]: 0.0815\n",
      "Loss at epoch [169 / 300]: 0.08146\n",
      "Loss at epoch [170 / 300]: 0.08153\n",
      "Loss at epoch [171 / 300]: 0.08152\n",
      "Loss at epoch [172 / 300]: 0.08147\n",
      "Loss at epoch [173 / 300]: 0.0815\n",
      "Loss at epoch [174 / 300]: 0.08143\n",
      "Loss at epoch [175 / 300]: 0.08142\n",
      "Loss at epoch [176 / 300]: 0.08144\n",
      "Loss at epoch [177 / 300]: 0.08139\n",
      "Loss at epoch [178 / 300]: 0.08142\n",
      "Loss at epoch [179 / 300]: 0.08142\n",
      "Loss at epoch [180 / 300]: 0.08139\n",
      "Loss at epoch [181 / 300]: 0.08132\n",
      "Loss at epoch [182 / 300]: 0.08136\n",
      "Loss at epoch [183 / 300]: 0.08134\n",
      "Loss at epoch [184 / 300]: 0.08132\n",
      "Loss at epoch [185 / 300]: 0.08132\n",
      "Loss at epoch [186 / 300]: 0.08131\n",
      "Loss at epoch [187 / 300]: 0.08134\n",
      "Loss at epoch [188 / 300]: 0.08126\n",
      "Loss at epoch [189 / 300]: 0.08136\n",
      "Loss at epoch [190 / 300]: 0.08126\n",
      "Loss at epoch [191 / 300]: 0.08123\n",
      "Loss at epoch [192 / 300]: 0.08121\n",
      "Loss at epoch [193 / 300]: 0.08122\n",
      "Loss at epoch [194 / 300]: 0.08116\n",
      "Loss at epoch [195 / 300]: 0.0812\n",
      "Loss at epoch [196 / 300]: 0.08113\n",
      "Loss at epoch [197 / 300]: 0.08118\n",
      "Loss at epoch [198 / 300]: 0.08114\n",
      "Loss at epoch [199 / 300]: 0.08112\n",
      "Loss at epoch [200 / 300]: 0.08116\n",
      "Loss at epoch [201 / 300]: 0.08108\n",
      "Loss at epoch [202 / 300]: 0.08104\n",
      "Loss at epoch [203 / 300]: 0.08104\n",
      "Loss at epoch [204 / 300]: 0.0811\n",
      "Loss at epoch [205 / 300]: 0.08103\n",
      "Loss at epoch [206 / 300]: 0.08114\n",
      "Loss at epoch [207 / 300]: 0.08106\n",
      "Loss at epoch [208 / 300]: 0.08106\n",
      "Loss at epoch [209 / 300]: 0.08096\n",
      "Loss at epoch [210 / 300]: 0.08109\n",
      "Loss at epoch [211 / 300]: 0.08103\n",
      "Loss at epoch [212 / 300]: 0.08109\n",
      "Loss at epoch [213 / 300]: 0.08099\n",
      "Loss at epoch [214 / 300]: 0.08108\n",
      "Loss at epoch [215 / 300]: 0.08112\n",
      "Loss at epoch [216 / 300]: 0.08113\n",
      "Loss at epoch [217 / 300]: 0.08108\n",
      "Loss at epoch [218 / 300]: 0.08106\n",
      "Loss at epoch [219 / 300]: 0.08108\n",
      "Loss at epoch [220 / 300]: 0.08102\n",
      "Loss at epoch [221 / 300]: 0.08099\n",
      "Loss at epoch [222 / 300]: 0.08107\n",
      "Loss at epoch [223 / 300]: 0.08105\n",
      "Loss at epoch [224 / 300]: 0.08113\n",
      "Loss at epoch [225 / 300]: 0.08101\n",
      "Loss at epoch [226 / 300]: 0.08102\n",
      "Loss at epoch [227 / 300]: 0.08101\n",
      "Loss at epoch [228 / 300]: 0.08104\n",
      "Loss at epoch [229 / 300]: 0.08112\n",
      "Loss at epoch [230 / 300]: 0.08101\n",
      "Loss at epoch [231 / 300]: 0.08105\n",
      "Loss at epoch [232 / 300]: 0.08106\n",
      "Loss at epoch [233 / 300]: 0.08099\n",
      "Loss at epoch [234 / 300]: 0.08105\n",
      "Loss at epoch [235 / 300]: 0.081\n",
      "Loss at epoch [236 / 300]: 0.08102\n",
      "Loss at epoch [237 / 300]: 0.08099\n",
      "Loss at epoch [238 / 300]: 0.08105\n",
      "Loss at epoch [239 / 300]: 0.08102\n",
      "Loss at epoch [240 / 300]: 0.08105\n",
      "Loss at epoch [241 / 300]: 0.08109\n",
      "Loss at epoch [242 / 300]: 0.08107\n",
      "Loss at epoch [243 / 300]: 0.08101\n",
      "Loss at epoch [244 / 300]: 0.08105\n",
      "Loss at epoch [245 / 300]: 0.08099\n",
      "Loss at epoch [246 / 300]: 0.081\n",
      "Loss at epoch [247 / 300]: 0.08103\n",
      "Loss at epoch [248 / 300]: 0.08099\n",
      "Loss at epoch [249 / 300]: 0.08104\n",
      "Loss at epoch [250 / 300]: 0.08105\n",
      "Loss at epoch [251 / 300]: 0.08106\n",
      "Loss at epoch [252 / 300]: 0.08102\n",
      "Loss at epoch [253 / 300]: 0.08108\n",
      "Loss at epoch [254 / 300]: 0.08099\n",
      "Loss at epoch [255 / 300]: 0.08095\n",
      "Loss at epoch [256 / 300]: 0.08099\n",
      "Loss at epoch [257 / 300]: 0.08104\n",
      "Loss at epoch [258 / 300]: 0.08092\n",
      "Loss at epoch [259 / 300]: 0.08099\n",
      "Loss at epoch [260 / 300]: 0.08097\n",
      "Loss at epoch [261 / 300]: 0.08098\n",
      "Loss at epoch [262 / 300]: 0.08101\n",
      "Loss at epoch [263 / 300]: 0.08093\n",
      "Loss at epoch [264 / 300]: 0.081\n",
      "Loss at epoch [265 / 300]: 0.08095\n",
      "Loss at epoch [266 / 300]: 0.08101\n",
      "Loss at epoch [267 / 300]: 0.08103\n",
      "Loss at epoch [268 / 300]: 0.08091\n",
      "Loss at epoch [269 / 300]: 0.08102\n",
      "Loss at epoch [270 / 300]: 0.08095\n",
      "Loss at epoch [271 / 300]: 0.08099\n",
      "Loss at epoch [272 / 300]: 0.08098\n",
      "Loss at epoch [273 / 300]: 0.08102\n",
      "Loss at epoch [274 / 300]: 0.08101\n",
      "Loss at epoch [275 / 300]: 0.08097\n",
      "Loss at epoch [276 / 300]: 0.08104\n",
      "Loss at epoch [277 / 300]: 0.08103\n",
      "Loss at epoch [278 / 300]: 0.081\n",
      "Loss at epoch [279 / 300]: 0.08101\n",
      "Loss at epoch [280 / 300]: 0.08101\n",
      "Loss at epoch [281 / 300]: 0.08095\n",
      "Loss at epoch [282 / 300]: 0.08101\n",
      "Loss at epoch [283 / 300]: 0.08095\n",
      "Loss at epoch [284 / 300]: 0.08097\n",
      "Loss at epoch [285 / 300]: 0.081\n",
      "Loss at epoch [286 / 300]: 0.08099\n",
      "Loss at epoch [287 / 300]: 0.08092\n",
      "Loss at epoch [288 / 300]: 0.08098\n",
      "Loss at epoch [289 / 300]: 0.08094\n",
      "Loss at epoch [290 / 300]: 0.08094\n",
      "Loss at epoch [291 / 300]: 0.08091\n",
      "Loss at epoch [292 / 300]: 0.08093\n",
      "Loss at epoch [293 / 300]: 0.08093\n",
      "Loss at epoch [294 / 300]: 0.08098\n",
      "Loss at epoch [295 / 300]: 0.08098\n",
      "Loss at epoch [296 / 300]: 0.08087\n",
      "Loss at epoch [297 / 300]: 0.08096\n",
      "Loss at epoch [298 / 300]: 0.08098\n",
      "Loss at epoch [299 / 300]: 0.081\n",
      "Loss at epoch [300 / 300]: 0.08095\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "loss_ = nn.MSELoss()\n",
    "# Initialize architecture of our Auto-Encoder\n",
    "model = AutoEncoder(input_size=X.shape[1], \n",
    "                    hidden_sizes=[500, 500, 2000, 10],\n",
    "                   # Prevent overfitting by deactivating 20% of the neurons during training\n",
    "                    dropout_rate=0.2 \n",
    "                   ).cuda()\n",
    "\n",
    "# Activate training mode\n",
    "model.train()\n",
    "\n",
    "# We could restore a model to continue training from a checkpoint\n",
    "#model = torch.load(\"./torch_models/autoencoder\")\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.1\n",
    "\n",
    "# Use Stochastic Gradient Descent as optimizer with momentum 0.9\n",
    "optimizer = torch.optim.SGD(lr=lr, \n",
    "                            momentum=0.9,\n",
    "                            params=model.parameters())\n",
    "\n",
    "# reduce learning rate as training continues\n",
    "scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                  step_size=100,\n",
    "                                  gamma=0.1)\n",
    "\n",
    "n_epochs = 300\n",
    "eval_every = 1\n",
    "best_loss = np.infty\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    # Iterate over data in batches\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # Transform input batch data\n",
    "        x_batch = x_batch.cuda()\n",
    "        x_batch = x_batch.view(x_batch.shape[0], -1)\n",
    "\n",
    "        # Apply AutoEncoder model\n",
    "        output = model(x_batch)[1]\n",
    "\n",
    "        # Calculate the reconstruction loss\n",
    "        loss = loss_(output, x_batch)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    mean_loss = np.round(np.mean(losses),5)\n",
    "    print(f\"Loss at epoch [{epoch+1} / {n_epochs}]: {mean_loss}\")\n",
    "\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = loss\n",
    "        # Store the best model\n",
    "        torch.save(model, \"./torch_models/autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc91791-940b-4069-a2db-589e96e05e40",
   "metadata": {},
   "source": [
    "#### Fine-Tune Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ef42c6-2d99-4f61-9479-f19fd8a121d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch [1 / 300]: 0.07107\n",
      "Loss at epoch [2 / 300]: 0.06741\n",
      "Loss at epoch [3 / 300]: 0.06542\n",
      "Loss at epoch [4 / 300]: 0.06411\n",
      "Loss at epoch [5 / 300]: 0.06315\n",
      "Loss at epoch [6 / 300]: 0.06241\n",
      "Loss at epoch [7 / 300]: 0.06179\n",
      "Loss at epoch [8 / 300]: 0.06127\n",
      "Loss at epoch [9 / 300]: 0.06082\n",
      "Loss at epoch [10 / 300]: 0.06042\n",
      "Loss at epoch [11 / 300]: 0.06006\n",
      "Loss at epoch [12 / 300]: 0.05973\n",
      "Loss at epoch [13 / 300]: 0.05942\n",
      "Loss at epoch [14 / 300]: 0.05914\n",
      "Loss at epoch [15 / 300]: 0.05887\n",
      "Loss at epoch [16 / 300]: 0.05862\n",
      "Loss at epoch [17 / 300]: 0.05838\n",
      "Loss at epoch [18 / 300]: 0.05815\n",
      "Loss at epoch [19 / 300]: 0.05793\n",
      "Loss at epoch [20 / 300]: 0.05773\n",
      "Loss at epoch [21 / 300]: 0.05753\n",
      "Loss at epoch [22 / 300]: 0.05733\n",
      "Loss at epoch [23 / 300]: 0.05715\n",
      "Loss at epoch [24 / 300]: 0.05697\n",
      "Loss at epoch [25 / 300]: 0.05679\n",
      "Loss at epoch [26 / 300]: 0.05662\n",
      "Loss at epoch [27 / 300]: 0.05646\n",
      "Loss at epoch [28 / 300]: 0.0563\n",
      "Loss at epoch [29 / 300]: 0.05615\n",
      "Loss at epoch [30 / 300]: 0.05599\n",
      "Loss at epoch [31 / 300]: 0.05585\n",
      "Loss at epoch [32 / 300]: 0.0557\n",
      "Loss at epoch [33 / 300]: 0.05556\n",
      "Loss at epoch [34 / 300]: 0.05542\n",
      "Loss at epoch [35 / 300]: 0.05529\n",
      "Loss at epoch [36 / 300]: 0.05516\n",
      "Loss at epoch [37 / 300]: 0.05503\n",
      "Loss at epoch [38 / 300]: 0.0549\n",
      "Loss at epoch [39 / 300]: 0.05477\n",
      "Loss at epoch [40 / 300]: 0.05465\n",
      "Loss at epoch [41 / 300]: 0.05453\n",
      "Loss at epoch [42 / 300]: 0.05441\n",
      "Loss at epoch [43 / 300]: 0.0543\n",
      "Loss at epoch [44 / 300]: 0.05418\n",
      "Loss at epoch [45 / 300]: 0.05407\n",
      "Loss at epoch [46 / 300]: 0.05396\n",
      "Loss at epoch [47 / 300]: 0.05385\n",
      "Loss at epoch [48 / 300]: 0.05374\n",
      "Loss at epoch [49 / 300]: 0.05364\n",
      "Loss at epoch [50 / 300]: 0.05353\n",
      "Loss at epoch [51 / 300]: 0.05343\n",
      "Loss at epoch [52 / 300]: 0.05333\n",
      "Loss at epoch [53 / 300]: 0.05323\n",
      "Loss at epoch [54 / 300]: 0.05313\n",
      "Loss at epoch [55 / 300]: 0.05303\n",
      "Loss at epoch [56 / 300]: 0.05294\n",
      "Loss at epoch [57 / 300]: 0.05284\n",
      "Loss at epoch [58 / 300]: 0.05275\n",
      "Loss at epoch [59 / 300]: 0.05266\n",
      "Loss at epoch [60 / 300]: 0.05257\n",
      "Loss at epoch [61 / 300]: 0.05248\n",
      "Loss at epoch [62 / 300]: 0.05239\n",
      "Loss at epoch [63 / 300]: 0.0523\n",
      "Loss at epoch [64 / 300]: 0.05222\n",
      "Loss at epoch [65 / 300]: 0.05213\n",
      "Loss at epoch [66 / 300]: 0.05205\n",
      "Loss at epoch [67 / 300]: 0.05196\n",
      "Loss at epoch [68 / 300]: 0.05188\n",
      "Loss at epoch [69 / 300]: 0.0518\n",
      "Loss at epoch [70 / 300]: 0.05172\n",
      "Loss at epoch [71 / 300]: 0.05164\n",
      "Loss at epoch [72 / 300]: 0.05156\n",
      "Loss at epoch [73 / 300]: 0.05148\n",
      "Loss at epoch [74 / 300]: 0.05141\n",
      "Loss at epoch [75 / 300]: 0.05133\n",
      "Loss at epoch [76 / 300]: 0.05126\n",
      "Loss at epoch [77 / 300]: 0.05118\n",
      "Loss at epoch [78 / 300]: 0.05111\n",
      "Loss at epoch [79 / 300]: 0.05103\n",
      "Loss at epoch [80 / 300]: 0.05096\n",
      "Loss at epoch [81 / 300]: 0.05089\n",
      "Loss at epoch [82 / 300]: 0.05082\n",
      "Loss at epoch [83 / 300]: 0.05075\n",
      "Loss at epoch [84 / 300]: 0.05068\n",
      "Loss at epoch [85 / 300]: 0.05061\n",
      "Loss at epoch [86 / 300]: 0.05054\n",
      "Loss at epoch [87 / 300]: 0.05047\n",
      "Loss at epoch [88 / 300]: 0.05041\n",
      "Loss at epoch [89 / 300]: 0.05034\n",
      "Loss at epoch [90 / 300]: 0.05027\n",
      "Loss at epoch [91 / 300]: 0.05021\n",
      "Loss at epoch [92 / 300]: 0.05014\n",
      "Loss at epoch [93 / 300]: 0.05008\n",
      "Loss at epoch [94 / 300]: 0.05002\n",
      "Loss at epoch [95 / 300]: 0.04995\n",
      "Loss at epoch [96 / 300]: 0.04989\n",
      "Loss at epoch [97 / 300]: 0.04983\n",
      "Loss at epoch [98 / 300]: 0.04977\n",
      "Loss at epoch [99 / 300]: 0.04971\n",
      "Loss at epoch [100 / 300]: 0.04965\n",
      "Loss at epoch [101 / 300]: 0.04959\n",
      "Loss at epoch [102 / 300]: 0.04953\n",
      "Loss at epoch [103 / 300]: 0.04947\n",
      "Loss at epoch [104 / 300]: 0.04941\n",
      "Loss at epoch [105 / 300]: 0.04935\n",
      "Loss at epoch [106 / 300]: 0.0493\n",
      "Loss at epoch [107 / 300]: 0.04924\n",
      "Loss at epoch [108 / 300]: 0.04918\n",
      "Loss at epoch [109 / 300]: 0.04913\n",
      "Loss at epoch [110 / 300]: 0.04907\n",
      "Loss at epoch [111 / 300]: 0.04902\n",
      "Loss at epoch [112 / 300]: 0.04896\n",
      "Loss at epoch [113 / 300]: 0.04891\n",
      "Loss at epoch [114 / 300]: 0.04885\n",
      "Loss at epoch [115 / 300]: 0.0488\n",
      "Loss at epoch [116 / 300]: 0.04875\n",
      "Loss at epoch [117 / 300]: 0.04869\n",
      "Loss at epoch [118 / 300]: 0.04864\n",
      "Loss at epoch [119 / 300]: 0.04859\n",
      "Loss at epoch [120 / 300]: 0.04854\n",
      "Loss at epoch [121 / 300]: 0.04849\n",
      "Loss at epoch [122 / 300]: 0.04844\n",
      "Loss at epoch [123 / 300]: 0.04838\n",
      "Loss at epoch [124 / 300]: 0.04833\n",
      "Loss at epoch [125 / 300]: 0.04829\n",
      "Loss at epoch [126 / 300]: 0.04824\n",
      "Loss at epoch [127 / 300]: 0.04819\n",
      "Loss at epoch [128 / 300]: 0.04814\n",
      "Loss at epoch [129 / 300]: 0.04809\n",
      "Loss at epoch [130 / 300]: 0.04804\n",
      "Loss at epoch [131 / 300]: 0.04799\n",
      "Loss at epoch [132 / 300]: 0.04795\n",
      "Loss at epoch [133 / 300]: 0.0479\n",
      "Loss at epoch [134 / 300]: 0.04785\n",
      "Loss at epoch [135 / 300]: 0.04781\n",
      "Loss at epoch [136 / 300]: 0.04776\n",
      "Loss at epoch [137 / 300]: 0.04771\n",
      "Loss at epoch [138 / 300]: 0.04767\n",
      "Loss at epoch [139 / 300]: 0.04762\n",
      "Loss at epoch [140 / 300]: 0.04758\n",
      "Loss at epoch [141 / 300]: 0.04753\n",
      "Loss at epoch [142 / 300]: 0.04749\n",
      "Loss at epoch [143 / 300]: 0.04745\n",
      "Loss at epoch [144 / 300]: 0.0474\n",
      "Loss at epoch [145 / 300]: 0.04736\n",
      "Loss at epoch [146 / 300]: 0.04732\n",
      "Loss at epoch [147 / 300]: 0.04727\n",
      "Loss at epoch [148 / 300]: 0.04723\n",
      "Loss at epoch [149 / 300]: 0.04719\n",
      "Loss at epoch [150 / 300]: 0.04715\n",
      "Loss at epoch [151 / 300]: 0.0471\n",
      "Loss at epoch [152 / 300]: 0.04706\n",
      "Loss at epoch [153 / 300]: 0.04702\n",
      "Loss at epoch [154 / 300]: 0.04698\n",
      "Loss at epoch [155 / 300]: 0.04694\n",
      "Loss at epoch [156 / 300]: 0.0469\n",
      "Loss at epoch [157 / 300]: 0.04686\n",
      "Loss at epoch [158 / 300]: 0.04682\n",
      "Loss at epoch [159 / 300]: 0.04678\n",
      "Loss at epoch [160 / 300]: 0.04674\n",
      "Loss at epoch [161 / 300]: 0.0467\n",
      "Loss at epoch [162 / 300]: 0.04666\n",
      "Loss at epoch [163 / 300]: 0.04662\n",
      "Loss at epoch [164 / 300]: 0.04658\n",
      "Loss at epoch [165 / 300]: 0.04654\n",
      "Loss at epoch [166 / 300]: 0.0465\n",
      "Loss at epoch [167 / 300]: 0.04646\n",
      "Loss at epoch [168 / 300]: 0.04643\n",
      "Loss at epoch [169 / 300]: 0.04639\n",
      "Loss at epoch [170 / 300]: 0.04635\n",
      "Loss at epoch [171 / 300]: 0.04631\n",
      "Loss at epoch [172 / 300]: 0.04628\n",
      "Loss at epoch [173 / 300]: 0.04624\n",
      "Loss at epoch [174 / 300]: 0.0462\n",
      "Loss at epoch [175 / 300]: 0.04617\n",
      "Loss at epoch [176 / 300]: 0.04613\n",
      "Loss at epoch [177 / 300]: 0.04609\n",
      "Loss at epoch [178 / 300]: 0.04606\n",
      "Loss at epoch [179 / 300]: 0.04602\n",
      "Loss at epoch [180 / 300]: 0.04599\n",
      "Loss at epoch [181 / 300]: 0.04595\n",
      "Loss at epoch [182 / 300]: 0.04592\n",
      "Loss at epoch [183 / 300]: 0.04588\n",
      "Loss at epoch [184 / 300]: 0.04585\n",
      "Loss at epoch [185 / 300]: 0.04581\n",
      "Loss at epoch [186 / 300]: 0.04578\n",
      "Loss at epoch [187 / 300]: 0.04574\n",
      "Loss at epoch [188 / 300]: 0.04571\n",
      "Loss at epoch [189 / 300]: 0.04567\n",
      "Loss at epoch [190 / 300]: 0.04564\n",
      "Loss at epoch [191 / 300]: 0.04561\n",
      "Loss at epoch [192 / 300]: 0.04557\n",
      "Loss at epoch [193 / 300]: 0.04554\n",
      "Loss at epoch [194 / 300]: 0.04551\n",
      "Loss at epoch [195 / 300]: 0.04547\n",
      "Loss at epoch [196 / 300]: 0.04544\n",
      "Loss at epoch [197 / 300]: 0.04541\n",
      "Loss at epoch [198 / 300]: 0.04538\n",
      "Loss at epoch [199 / 300]: 0.04534\n",
      "Loss at epoch [200 / 300]: 0.04531\n",
      "Loss at epoch [201 / 300]: 0.04528\n",
      "Loss at epoch [202 / 300]: 0.04525\n",
      "Loss at epoch [203 / 300]: 0.04521\n",
      "Loss at epoch [204 / 300]: 0.04518\n",
      "Loss at epoch [205 / 300]: 0.04515\n",
      "Loss at epoch [206 / 300]: 0.04512\n",
      "Loss at epoch [207 / 300]: 0.04509\n",
      "Loss at epoch [208 / 300]: 0.04506\n",
      "Loss at epoch [209 / 300]: 0.04503\n",
      "Loss at epoch [210 / 300]: 0.045\n",
      "Loss at epoch [211 / 300]: 0.04497\n",
      "Loss at epoch [212 / 300]: 0.04493\n",
      "Loss at epoch [213 / 300]: 0.0449\n",
      "Loss at epoch [214 / 300]: 0.04487\n",
      "Loss at epoch [215 / 300]: 0.04484\n",
      "Loss at epoch [216 / 300]: 0.04481\n",
      "Loss at epoch [217 / 300]: 0.04478\n",
      "Loss at epoch [218 / 300]: 0.04475\n",
      "Loss at epoch [219 / 300]: 0.04473\n",
      "Loss at epoch [220 / 300]: 0.0447\n",
      "Loss at epoch [221 / 300]: 0.04467\n",
      "Loss at epoch [222 / 300]: 0.04464\n",
      "Loss at epoch [223 / 300]: 0.04461\n",
      "Loss at epoch [224 / 300]: 0.04458\n",
      "Loss at epoch [225 / 300]: 0.04455\n",
      "Loss at epoch [226 / 300]: 0.04452\n",
      "Loss at epoch [227 / 300]: 0.04449\n",
      "Loss at epoch [228 / 300]: 0.04446\n",
      "Loss at epoch [229 / 300]: 0.04444\n",
      "Loss at epoch [230 / 300]: 0.04441\n",
      "Loss at epoch [231 / 300]: 0.04438\n",
      "Loss at epoch [232 / 300]: 0.04435\n",
      "Loss at epoch [233 / 300]: 0.04432\n",
      "Loss at epoch [234 / 300]: 0.0443\n",
      "Loss at epoch [235 / 300]: 0.04427\n",
      "Loss at epoch [236 / 300]: 0.04424\n",
      "Loss at epoch [237 / 300]: 0.04421\n",
      "Loss at epoch [238 / 300]: 0.04419\n",
      "Loss at epoch [239 / 300]: 0.04416\n",
      "Loss at epoch [240 / 300]: 0.04413\n",
      "Loss at epoch [241 / 300]: 0.04411\n",
      "Loss at epoch [242 / 300]: 0.04408\n",
      "Loss at epoch [243 / 300]: 0.04405\n",
      "Loss at epoch [244 / 300]: 0.04402\n",
      "Loss at epoch [245 / 300]: 0.044\n",
      "Loss at epoch [246 / 300]: 0.04397\n",
      "Loss at epoch [247 / 300]: 0.04395\n",
      "Loss at epoch [248 / 300]: 0.04392\n",
      "Loss at epoch [249 / 300]: 0.04389\n",
      "Loss at epoch [250 / 300]: 0.04387\n",
      "Loss at epoch [251 / 300]: 0.04384\n",
      "Loss at epoch [252 / 300]: 0.04382\n",
      "Loss at epoch [253 / 300]: 0.04379\n",
      "Loss at epoch [254 / 300]: 0.04376\n",
      "Loss at epoch [255 / 300]: 0.04374\n",
      "Loss at epoch [256 / 300]: 0.04371\n",
      "Loss at epoch [257 / 300]: 0.04369\n",
      "Loss at epoch [258 / 300]: 0.04366\n",
      "Loss at epoch [259 / 300]: 0.04364\n",
      "Loss at epoch [260 / 300]: 0.04361\n",
      "Loss at epoch [261 / 300]: 0.04359\n",
      "Loss at epoch [262 / 300]: 0.04356\n",
      "Loss at epoch [263 / 300]: 0.04354\n",
      "Loss at epoch [264 / 300]: 0.04351\n",
      "Loss at epoch [265 / 300]: 0.04349\n",
      "Loss at epoch [266 / 300]: 0.04346\n",
      "Loss at epoch [267 / 300]: 0.04344\n",
      "Loss at epoch [268 / 300]: 0.04341\n",
      "Loss at epoch [269 / 300]: 0.04339\n",
      "Loss at epoch [270 / 300]: 0.04337\n",
      "Loss at epoch [271 / 300]: 0.04334\n",
      "Loss at epoch [272 / 300]: 0.04332\n",
      "Loss at epoch [273 / 300]: 0.04329\n",
      "Loss at epoch [274 / 300]: 0.04327\n",
      "Loss at epoch [275 / 300]: 0.04325\n",
      "Loss at epoch [276 / 300]: 0.04322\n",
      "Loss at epoch [277 / 300]: 0.0432\n",
      "Loss at epoch [278 / 300]: 0.04318\n",
      "Loss at epoch [279 / 300]: 0.04315\n",
      "Loss at epoch [280 / 300]: 0.04313\n",
      "Loss at epoch [281 / 300]: 0.04311\n",
      "Loss at epoch [282 / 300]: 0.04308\n",
      "Loss at epoch [283 / 300]: 0.04306\n",
      "Loss at epoch [284 / 300]: 0.04304\n",
      "Loss at epoch [285 / 300]: 0.04301\n",
      "Loss at epoch [286 / 300]: 0.04299\n",
      "Loss at epoch [287 / 300]: 0.04297\n",
      "Loss at epoch [288 / 300]: 0.04295\n",
      "Loss at epoch [289 / 300]: 0.04292\n",
      "Loss at epoch [290 / 300]: 0.0429\n",
      "Loss at epoch [291 / 300]: 0.04288\n",
      "Loss at epoch [292 / 300]: 0.04286\n",
      "Loss at epoch [293 / 300]: 0.04283\n",
      "Loss at epoch [294 / 300]: 0.04281\n",
      "Loss at epoch [295 / 300]: 0.04279\n",
      "Loss at epoch [296 / 300]: 0.04277\n",
      "Loss at epoch [297 / 300]: 0.04274\n",
      "Loss at epoch [298 / 300]: 0.04272\n",
      "Loss at epoch [299 / 300]: 0.0427\n",
      "Loss at epoch [300 / 300]: 0.04268\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./torch_models/autoencoder\")\n",
    "\n",
    "# Inference Mode for fine-tuning\n",
    "model.eval()\n",
    "\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.SGD(lr=lr, \n",
    "                            momentum=0.9,\n",
    "                            params=model.parameters()\n",
    "                           )\n",
    "n_epochs = 300\n",
    "eval_every = 1\n",
    "best_loss = np.infty\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch = x_batch.cuda()\n",
    "        x_batch = x_batch.view(x_batch.shape[0], -1)\n",
    "        \n",
    "        \n",
    "        output = model(x_batch)[1]\n",
    "        \n",
    "        loss = loss_(output, x_batch)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_loss = np.round(np.mean(losses),5)\n",
    "    print(f\"Loss at epoch [{epoch+1} / {n_epochs}]: {mean_loss}\")\n",
    "    torch.save(model, \"./torch_models/autoencoder-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0847a-5cb2-496c-a582-f8e691baa724",
   "metadata": {},
   "source": [
    "#### Apply embedding on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de956cdd-6e91-42ca-b327-8f1aa1f03d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./torch_models/autoencoder-finetuned\")\n",
    "X_train_embedded = model(Tensor(X).cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a3800f4-d874-45ee-a080-16c8607e949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70000, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6b7d5-ab2b-4a50-9cdf-09383abd51be",
   "metadata": {},
   "source": [
    "#### Apply k-Means on the embedded data from Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1157638-8a5e-453b-86d3-39ea73a81d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, random_state=rs)\n",
    "y_pred_train_AE = kmeans.fit_predict(X_train_embedded.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d0e847-dcc4-40f9-94a9-b3939f6584ac",
   "metadata": {},
   "source": [
    "#### Evaluate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "627ca9cd-e95e-490a-aea5-7001c132dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Auto-Encoder:\n",
      "AMI: 0.728146922504046\n",
      "ARI: 0.6625540981565513\n",
      "Accuracy: 0.7561714285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Auto-Encoder:\")\n",
    "print(f\"AMI: {adjusted_mutual_info_score(y, y_pred_train_AE)}\")\n",
    "print(f\"ARI: {adjusted_rand_score(y, y_pred_train_AE)}\")\n",
    "print(f\"Accuracy: {clustering_accuracy(y, y_pred_train_AE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0779f622-01dc-4390-bbd5-e4bb8b9c71e9",
   "metadata": {},
   "source": [
    "Using Auto-Encoders, we are able to increase the accuracy of the simple k-Means clustering algorithm by more than 20%-points!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
